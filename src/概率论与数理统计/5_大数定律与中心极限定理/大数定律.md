
**大数定律**又称**大数法则**、**大数律**，是描述相当多次数重复实验的结果的定律。根据这个定律知道，样本数量越多，则其算术平均值就有越高的概率接近期望值。
它说明了试验的频率收敛于概率这一事实，或者说从理论上说明了随机变量依概率收敛于它的期望

### 切比雪夫大数定理
#### 描述一
假设$\{X_{n}\}(n=1,2,...)$是相互独立的随机变量序列,
若方差$D(X_{i})(i\geq 1)$存在 且一致有上界, 即存在常数$C$,使$D(X_{i})\leq C$对一切$i\geq 1$均成立,则$\{X_{n}\}$服从大数定律:
$$
\frac{1}{n}\sum_{i=1}^{n} X_{i} \xrightarrow{P} \frac{1}{n}E(X_{i})
$$

换言之，在定理条件下，当 ${\displaystyle n}$ 无限变大时， ${\displaystyle n}$ 个随机变量的算术平均将变成一个常数
#### 描述二
设 $\{X_i\}$为两两独立的随机变量序列，若$E(X_i)=\mu_i, i=1,2,\cdots$ ，$D(X_i)=\sigma_i^2$ 存在，且$\sigma_i^2,\le c$，则
$$
\lim_{n\to\infty}P\left\{\left|\frac{1}{n}\sum_{i=1}^nX_i-\frac{1}{n}\sum_{i=1}^nE(X_i)\right|<\epsilon\right\}=1
$$
#### 证明
$$
\begin{align*}1\ge P\left\{\left|\frac{1}{n}\sum_{i=1}^nX_i-\frac{1}{n}\sum_{i=1}^nE(X_i)\right|<\epsilon\right\}&\ge 1-\frac{D\left(\frac{1}{n}\sum_{i=1}^nX_i\right)}{\epsilon^2}\\ &=1-\frac{1}{n^2}\frac{\sum_{i=1}^nD(X_i)}{\epsilon^2}\\ &\ge 1-\frac{1}{n^2}\frac{nc}{\epsilon^2}\\ &=1-\frac{c}{n\epsilon^2}\\ &\to 1\end{align*}
$$

### 伯努利大数定律
假设$u_{n}$是$n$重伯努利试验中 事件$A$发生的次数, 在每次试验中事件$A$发生的概率为$p(0<p<1)$,则$\displaystyle{\frac{u_{n}}{n}\xrightarrow{P}p}$,即对任意$\varepsilon>0$,有
$$
\lim_{ n \to \infty } P\left( \Bigg| \frac{u_{n}}{n}-p \Bigg| <\varepsilon \right )=1
$$
也就是说， $A$出现的频率收敛于它的概率

### 弱大数定律(辛钦大数定理)
假设$\{X_{n}\}$是独立同分布的随机变量序列, 若$E(X_{i})=u(i=1,2,...)$存在,则$\displaystyle{\frac{1}{n}\sum_{i=1}^{n}X_{i}\xrightarrow{P}u}$,即对任意$\varepsilon>0$,有
$$
\lim_{ n \to \infty } \left( \Bigg | \frac{1}{n}\sum_{i=1}^{n} X_{i}-u \Bigg|<\varepsilon  \right)=1
$$



# 参考
[大数定律 - 维基百科，自由的百科全书](https://zh.wikipedia.org/wiki/%E5%A4%A7%E6%95%B8%E6%B3%95%E5%89%87)
[大数定律 – 四都教育](https://www.sudoedu.com/%e6%a6%82%e7%8e%87%e7%bb%9f%e8%ae%a1%e8%a7%86%e9%a2%91%e8%af%be%e7%a8%8b/%e6%a6%82%e7%8e%87%e8%ae%ba%e7%9a%84%e6%9e%81%e9%99%90%e7%90%86%e8%ae%ba/%e5%a4%a7%e6%95%b0%e5%ae%9a%e5%be%8b-4/)
