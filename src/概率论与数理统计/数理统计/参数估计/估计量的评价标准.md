
我们从前面的点估计方法知道，参数的点估计方法有多种。原则上，任何统计量都可以作为未知参数的点估计，那么问题来了：哪一种估计更好、更接近于未知参数呢？这里我们有一些评估的标准，主要的有三个：无偏性、有效性和相合性（也叫一致性）
是用于衡量 估计量$\hat{\theta}$与未知参数$\theta$的 接近程度 的

### 无偏性
若参数$\theta$的估计量$\hat{\theta}=\hat{\theta}(X_{1},...,X_{n})$对一切$n$及$\theta\in I$,有$E(\hat{\theta})=\theta$ ,则称$\hat{\theta}$为$\theta$的 **无偏估计量**


### 有效性(最小方差性)
设$\hat{\theta_{1}}=\hat{\theta_{1}}(X_{1},...,X_{n})$与$\hat{\theta_{2}}=\hat{\theta_{2}}(X_{1},...,X_{n})$都是$\theta$的无偏估计量,
若$D(\hat{\theta_1})\le D(\hat{\theta_2})$，称$\hat{\theta_1}$比$\hat{\theta_2}$**更有效**

### 相合性(一致性)
设$\hat{\theta}=\hat{\theta}(X_{1},...,X_{n})$为未知参数$\theta$的估计量,
若对任意的$\varepsilon>0$,有
$$
\lim_{n\to \infty}P\{|\hat{\theta}-\theta|<\varepsilon\}=1
$$
则称$\hat{\theta}$是$\theta$的**相合估计量(一致估计量)**
可以使用[[src/概率论与数理统计/随机变量的数字特征/切比雪夫不等式|切比雪夫不等式]] + 夹逼定理 去证明相合性
也可以利用 [[src/概率论与数理统计/大数定律与中心极限定理/大数定律#弱大数定律(辛钦大数定理)|弱大数定律(辛钦大数定理)]] 

